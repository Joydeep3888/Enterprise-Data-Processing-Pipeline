{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "334718b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Data:\n",
      "+--------------------+-----------------+---+-----------+--------------+------------+-------------+----------+----------+\n",
      "|           policy_id|    customer_name|age|policy_type|premium_amount|claim_status|         city|     state|issue_date|\n",
      "+--------------------+-----------------+---+-----------+--------------+------------+-------------+----------+----------+\n",
      "|7c5cb5e9-a58b-4e4...|       Ryan White| 49|     Travel|           913|    Approved|     Kingbury|   Georgia|2017-12-04|\n",
      "|67ee8fe8-86b0-4c3...|      Joseph Cook| 65|     Travel|          3819|     Pending|  Williamtown|California|2017-05-04|\n",
      "|91628a6a-81dc-49e...|Christopher Miles| 38|       Life|           204|     Pending|        Wuton|  Illinois|2025-01-04|\n",
      "|9c779940-a844-406...|        Blake Cox| 64|       Auto|           974|      Denied|New Rickyside|California|2022-11-12|\n",
      "|ccd80a86-1ef4-42c...| Carolyn Stephens| 71|       Life|          3754|     Pending|   Port Diane|  New York|2017-12-09|\n",
      "+--------------------+-----------------+---+-----------+--------------+------------+-------------+----------+----------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Transformed Data:\n",
      "+----------+--------------------+-----------------+---+-----------+--------------+------------+-------------+----------+-----------+---------------+----------+------------------+------------+-------------+\n",
      "|     state|           policy_id|    customer_name|age|policy_type|premium_amount|claim_status|         city|issue_date|  age_group|policy_duration|risk_score|       avg_premium|total_claims|total_premium|\n",
      "+----------+--------------------+-----------------+---+-----------+--------------+------------+-------------+----------+-----------+---------------+----------+------------------+------------+-------------+\n",
      "|   Georgia|7c5cb5e9-a58b-4e4...|       Ryan White| 49|     Travel|           913|    Approved|     Kingbury|2017-12-04|Middle-aged|              8|         3|2547.3735266008284|       12556|     31984822|\n",
      "|California|67ee8fe8-86b0-4c3...|      Joseph Cook| 65|     Travel|          3819|     Pending|  Williamtown|2017-05-04|     Senior|              8|         2|2547.5227714748785|       12340|     31436431|\n",
      "|  Illinois|91628a6a-81dc-49e...|Christopher Miles| 38|       Life|           204|     Pending|        Wuton|2025-01-04|Middle-aged|              0|         2| 2558.821805111821|       12520|     32036449|\n",
      "|California|9c779940-a844-406...|        Blake Cox| 64|       Auto|           974|      Denied|New Rickyside|2022-11-12|     Senior|              3|         1|2547.5227714748785|       12340|     31436431|\n",
      "|  New York|ccd80a86-1ef4-42c...| Carolyn Stephens| 71|       Life|          3754|     Pending|   Port Diane|2017-12-09|     Senior|              8|         2| 2550.390259635543|       12402|     31629940|\n",
      "+----------+--------------------+-----------------+---+-----------+--------------+------------+-------------+----------+-----------+---------------+----------+------------------+------------+-------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Table EDPP_processed_insurance_data created successfully.\n",
      " Successfully loaded 100 records into MySQL.\n",
      " MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, year, avg, count, sum\n",
    "import mysql.connector\n",
    "import os\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "import pandas as pd\n",
    "\n",
    "# AWS Configurations\n",
    "AWS_ACCESS_KEY = \"AKIAIOSFODNN7EXAMPLE\"\n",
    "AWS_SECRET_KEY = \"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\n",
    "S3_BUCKET_NAME = \"joydeep-data-bucket\"\n",
    "RAW_ZONE = \"raw/\"\n",
    "STAGING_ZONE = \"staging/\"\n",
    "\n",
    "# MySQL Database Configuration\n",
    "MYSQL_HOST = \"localhost\"\n",
    "MYSQL_DB = \"hr\"\n",
    "MYSQL_USER = \"root\"\n",
    "MYSQL_PASSWORD = \"1234\"\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ETL Pipeline\").getOrCreate()\n",
    "\n",
    "# Extract Data from CSV\n",
    "def extract_data():\n",
    "    \"\"\"Reads insurance data from CSV file\"\"\"\n",
    "    df = spark.read.csv(r\"C:\\Users\\hp\\OneDrive\\Desktop\\python\\Big_Data\\Cloned\\Project\\insurance_data.csv\", header=True, inferSchema=True)\n",
    "    print(\"Extracted Data:\")\n",
    "    df.show(5)\n",
    "    return df\n",
    "\n",
    "# Transform Data\n",
    "def transform_data(df):\n",
    "    \"\"\"Applies transformations including categorization, risk assessment, and aggregations\"\"\"\n",
    "    df = df.withColumn(\"age_group\", \n",
    "                       when(col(\"age\") < 30, \"Young\")\n",
    "                       .when((col(\"age\") >= 30) & (col(\"age\") < 50), \"Middle-aged\")\n",
    "                       .otherwise(\"Senior\"))\n",
    "    \n",
    "    df = df.withColumn(\"policy_duration\", year(lit(\"2025-01-01\")) - year(col(\"issue_date\")))\n",
    "    \n",
    "    df = df.withColumn(\"risk_score\", \n",
    "                       when(col(\"claim_status\") == \"Approved\", 3)\n",
    "                       .when(col(\"claim_status\") == \"Pending\", 2)\n",
    "                       .when(col(\"claim_status\") == \"Denied\", 1)\n",
    "                       .otherwise(0))\n",
    "    \n",
    "    premium_avg_df = df.groupBy(\"state\").agg(avg(\"premium_amount\").alias(\"avg_premium\"))\n",
    "    claims_count_df = df.groupBy(\"state\").agg(count(\"claim_status\").alias(\"total_claims\"))\n",
    "    total_premium_df = df.groupBy(\"state\").agg(sum(\"premium_amount\").alias(\"total_premium\"))\n",
    "    \n",
    "    df = df.join(premium_avg_df, on=\"state\", how=\"left\")\n",
    "    df = df.join(claims_count_df, on=\"state\", how=\"left\")\n",
    "    df = df.join(total_premium_df, on=\"state\", how=\"left\")\n",
    "    \n",
    "    print(\"Transformed Data:\")\n",
    "    df.show(5)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data(df):\n",
    "    \"\"\"Loads transformed PySpark DataFrame into MySQL Workbench efficiently.\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Establish MySQL Connection\n",
    "        connection = mysql.connector.connect(\n",
    "            host=MYSQL_HOST,\n",
    "            user=MYSQL_USER,\n",
    "            password=MYSQL_PASSWORD,\n",
    "            database=MYSQL_DB\n",
    "        )\n",
    "        cursor = connection.cursor()\n",
    "        \n",
    "        # Drop Table if Exists\n",
    "        cursor.execute(\"DROP TABLE IF EXISTS EDPP_processed_insurance_data\")\n",
    "        \n",
    "        # Create Table (if not exists)\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS EDPP_processed_insurance_data (\n",
    "                id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                customer_name VARCHAR(255),\n",
    "                age INT,\n",
    "                state VARCHAR(255),\n",
    "                issue_date DATE,\n",
    "                claim_status VARCHAR(50),\n",
    "                premium_amount FLOAT,\n",
    "                age_group VARCHAR(50),\n",
    "                policy_duration INT,\n",
    "                risk_score INT,\n",
    "                avg_premium FLOAT,\n",
    "                total_claims INT,\n",
    "                total_premium FLOAT\n",
    "            )\n",
    "        \"\"\")\n",
    "\n",
    "        # Convert PySpark DataFrame to Pandas DataFrame\n",
    "        df_pandas = df.toPandas()\n",
    "        \n",
    "        expected_columns = [\n",
    "            \"customer_name\", \"age\", \"state\", \"issue_date\", \"claim_status\", \"premium_amount\",\n",
    "            \"age_group\", \"policy_duration\", \"risk_score\", \"avg_premium\", \"total_claims\", \"total_premium\"\n",
    "        ]\n",
    "        \n",
    "        # Select only required columns & replace NaN with None for MySQL compatibility\n",
    "        df_pandas = df_pandas[expected_columns].where(pd.notna(df_pandas), None)\n",
    "\n",
    "        # Convert DataFrame to List of Tuples for Efficient Bulk Insert\n",
    "        data_to_insert = [tuple(row) for _, row in df_pandas.iterrows()]\n",
    "\n",
    "        insert_query = f\"\"\"\n",
    "            INSERT INTO EDPP_processed_insurance_data ({\", \".join(expected_columns)})\n",
    "            VALUES ({\", \".join([\"%s\"] * len(expected_columns))})\n",
    "        \"\"\"\n",
    "        print(\"Table EDPP_processed_insurance_data created successfully.\")\n",
    "            \n",
    "        # Insert only 100 records\n",
    "        if data_to_insert:\n",
    "            cursor.executemany(insert_query, data_to_insert[:100])\n",
    "            connection.commit()\n",
    "            print(f\" Successfully loaded {len(data_to_insert[:100])} records into MySQL.\")\n",
    "\n",
    "    except mysql.connector.Error as e:\n",
    "        print(f\" Error: {e}\")\n",
    "    \n",
    "    finally:\n",
    "        # Close Connections\n",
    "        if cursor:\n",
    "            cursor.close()\n",
    "        if connection:\n",
    "            connection.close()\n",
    "            print(\" MySQL connection closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    extracted_df = extract_data()\n",
    "    transformed_df = transform_data(extracted_df)\n",
    "    load_data(transformed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845946e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
